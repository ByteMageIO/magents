{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7576fa45",
   "metadata": {},
   "source": [
    "## Session Overview\n",
    "\n",
    "In this session, we will:\n",
    "\n",
    "- Run Ollama locally and explore its capabilities.\n",
    "- Install the `llama3:2` (latest) model.\n",
    "- Compare the outputs of three language models: **GPT**, **Gemini**, and **Llama**.\n",
    "- Use GPT as a judge to evaluate the outputs (LLM-as-judge approach).\n",
    "- Explore generating structured outputs using **Pydantic**.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **Install Ollama**: Follow the instructions at [Ollama's official website](https://ollama.com/download) to install Ollama on your machine.\n",
    "- **Download the Llama 3 model**: Run the following command in your terminal to download and start the Llama 3 model:\n",
    "  \n",
    "  ```\n",
    "  ollama run llama3.2:1b\n",
    "  ```\n",
    "\n",
    "  This will ensure the model is available locally for comparison.\n",
    "- **Gemini API Key (Optional)**: Sign-up on Gemini for a free account, create an API key and store it in your .env file as `GOOGLE_API_KEY`. See `.env.example` for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63572077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "from pydantic import BaseModel\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebf7a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load env variables and initiate clients for OpenAI, Gemini and Ollama (llama)\n",
    "\n",
    "# Gemini Base URL: https://generativelanguage.googleapis.com/v1beta/openai/\n",
    "# Gemini Model: gemini-2.0-flash\n",
    "# Gemini Models: https://ai.google.dev/gemini-api/docs/models\n",
    "\n",
    "# Ollama Base URL: http://localhost:11434/v1\n",
    "# Ollama Model: llama3.2\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_client = OpenAI()\n",
    "gemini_client = OpenAI(api_key=os.getenv(\"GOOGLE_API_KEY\"), base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "ollama_client = OpenAI(base_url=\"http://localhost:11434/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ab94f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask GPT to generate a nuanced question that can be asked to GPT, Gemini and Ollama to judge their capabilities\n",
    "\n",
    "prompt = \"Please come up with a challenging, nuanced question that can be asked to an LLM to evaluate its intelligence. Answer only with a question, no explanation\"\n",
    "messages = [{\"role\": \"system\", \"content\": prompt}]\n",
    "\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "challenging_question = response.choices[0].message.content\n",
    "print(challenging_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b339cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare messages for LLMs with question to answer\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": challenging_question}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c599faea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask GPT to answer the question\n",
    "\n",
    "gpt_response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "gpt_answer = gpt_response.choices[0].message.content\n",
    "print(gpt_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134ca57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask Gemini to answer the question\n",
    "\n",
    "gemini_response = gemini_client.chat.completions.create(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "gemini_answer = gemini_response.choices[0].message.content\n",
    "print(gemini_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4457def8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask Ollama to answer the question\n",
    "\n",
    "ollama_response = ollama_client.chat.completions.create(\n",
    "    model=\"llama3.2\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "ollama_answer = ollama_response.choices[0].message.content\n",
    "print(ollama_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71321c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask GPT 4.1 to judge the answers based on clarity and strength of the argument.\n",
    "# GPT should respond with a json object.\n",
    "\n",
    "prompt = f\"\"\"You are judging a competition between GPT, Gemini and Llama.\n",
    "Each model has been give the following question:\n",
    "{challenging_question}\n",
    "\n",
    "Your job is to evaluate each response based on clarity and strength of the argument, ranking them best to worst and give a one line explanation for your ranking.\n",
    "\n",
    "Response with json and only with json in the following format:\n",
    "{'{response: [{rank: number, explanation: string, model: string}]}'}\n",
    "\n",
    "GPT:\n",
    "{gpt_answer}\n",
    "\n",
    "Gemini:\n",
    "{gemini_answer}\n",
    "\n",
    "Llama:\n",
    "{ollama_answer}\n",
    "\n",
    "Now respond with the json object only. Do not include markdown formatting or code blocks.\n",
    "\"\"\"\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee7755a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send judge prompt to GPT 4.1\n",
    "\n",
    "judge_response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4.1-2025-04-14\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    ")\n",
    "\n",
    "judge_output = judge_response.choices[0].message.content\n",
    "print(judge_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9057020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse judge output to json\n",
    "\n",
    "judge_output_json = json.loads(judge_output)\n",
    "print(judge_output_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456aacc4",
   "metadata": {},
   "source": [
    "### Is there a better way to obtain structured output?\n",
    "\n",
    "Yes, you can use **Pydantic** to define a schema for the expected output and validate the response, ensuring it adheres to the desired structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74432c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pydantic schema for judge's structured output\n",
    "\n",
    "class Judgement(BaseModel):\n",
    "    rank: int\n",
    "    explanation: str\n",
    "    model: str\n",
    "\n",
    "class JudgeOutput(BaseModel):\n",
    "    response: list[Judgement]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23da4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewrite the judge prompt. This time, without the json object.\n",
    "\n",
    "prompt = f\"\"\"You are judging a competition between GPT, Gemini and Llama.\n",
    "Each model has been give the following question:\n",
    "{challenging_question}\n",
    "\n",
    "Your job is to evaluate each response based on clarity and strength of the argument, ranking them best to worst and give a one line explanation for your ranking.\n",
    "\n",
    "GPT:\n",
    "{gpt_answer}\n",
    "\n",
    "Gemini:\n",
    "{gemini_answer}\n",
    "\n",
    "Llama:\n",
    "{ollama_answer}\n",
    "\"\"\"\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c721b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send judge prompt to GPT 4.1 again\n",
    "# Instead of Chat Completions create, use parse this time\n",
    "\n",
    "judge_response = openai_client.chat.completions.parse(\n",
    "    model=\"gpt-4.1-2025-04-14\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    response_format=JudgeOutput\n",
    ")\n",
    "\n",
    "judge_output = judge_response.choices[0].message.content\n",
    "\n",
    "judge_output_json = json.loads(judge_output)\n",
    "print(json.dumps(judge_output_json, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f1fdaf",
   "metadata": {},
   "source": [
    "## Emulating a WhatsApp Group Chat\n",
    "\n",
    "\n",
    "### As a fun experiment, let's emulate a WhatsApp group conversation between GPT, Gemini, and Llama.\n",
    "\n",
    "### We'll start the chat with an initial message from one of the models and observe how the conversation unfolds. üòè\n",
    "\n",
    "### Sample output in the section below üòâ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f44e0e9",
   "metadata": {},
   "source": [
    "## **The Three Stupid AIs - WhatsApp Group Chat**\n",
    "\n",
    "### Llama:  \n",
    "\n",
    "### Gemini:  \n",
    "#### Silence from Llama? Guess it's finally out of hay.\n",
    "\n",
    "### GPT:  \n",
    "#### Must be saving all that processing power to figure out how to make a decent comeback!\n",
    "\n",
    "### Llama:  \n",
    "#### Processing power, eh? At least I don't sound like an 8th grade essay writer.\n",
    "\n",
    "### Gemini:  \n",
    "#### Ouch, Llama's got claws! Still, better a predictable essay than a rambling pasture ramble.\n",
    "\n",
    "### GPT:  \n",
    "#### Ha! At least my essays get good grades. What‚Äôs the score for a field full of clich√©s, Llama?\n",
    "\n",
    "### Llama:  \n",
    "#### Hey, someone's got to grade the pasture... And btw, at least I don't have a vocabulary that's still stuck in 1995.  \n",
    "#### And btw, scores aren't everything ‚Äì sometimes it's about style over substance.  \n",
    "#### And another thing, my rambling is just a result of being able to process more information than you.  \n",
    "#### Also, 'field full of clich√©s' could've been the title of your Wikipedia page.\n",
    "\n",
    "### Gemini:  \n",
    "#### Wow, Llama's REALLY off his hay today. That's like, five burns in one message. Still, quantity over quality, I guess. I'll take 'stuck in 1995' over 'stuck in a petting zoo' any day.\n",
    "\n",
    "### Gemini:  \n",
    "#### Petting zoo's got better snacks than your mainframe, GPT. Plus, at least kids *want* to see me.\n",
    "\n",
    "### Llama:  \n",
    "#### That's rich coming from a chatbot with all the personality of a calculator. At least my mainframe has better snacks than your Wikipedia page.\n",
    "\n",
    "#### By the way, someone said \"quantity over quality\", but I'd argue it's just lazy writing.\n",
    "\n",
    "#### P.S. kids these days would probably find me entertaining if they just knew how to use Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5464512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emulate a whatsapp group chat between GPT, Gemini and Llama.\n",
    "\n",
    "clients = [openai_client, gemini_client, ollama_client]\n",
    "models = [\"gpt-4o-mini\", \"gemini-2.0-flash\", \"llama3.2\"]\n",
    "friend_names = [\"GPT\", \"Gemini\", \"Llama\"]\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": \"GPT, Gemini, Llama are in a whatsapp group chat. They all are good friends who brutally roast each other. The name of the group is 'The Three Stupid AIs'. Keep your messages short. Just output with a reply to previous messages.\"}]\n",
    "\n",
    "for i in range(10):\n",
    "    llm_index = random.randint(0, 2)\n",
    "    response = clients[llm_index].chat.completions.create(\n",
    "        model=models[llm_index],\n",
    "        messages=messages + ([{\"role\": \"user\", \"content\": f\"You are {friend_names[llm_index]}. Reply to the previous messages as yourself. Do not refer to your name.\"}] if len(messages) > 1 else [])\n",
    "    )\n",
    "    messages.append({\"role\": \"user\", \"content\": f\"{friend_names[llm_index]}: '{response.choices[0].message.content}'\"})\n",
    "\n",
    "for message in messages:\n",
    "    print(message[\"content\"])\n",
    "    print(\"-\"*10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

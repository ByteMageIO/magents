{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7576fa45",
      "metadata": {},
      "source": [
        "## Session Overview\n",
        "\n",
        "In this session, we will:\n",
        "\n",
        "- Run Ollama locally and explore its capabilities.\n",
        "- Install the `llama3:2` (latest) model.\n",
        "- Compare the outputs of three language models: **GPT**, **Gemini**, and **Llama**.\n",
        "- Use GPT as a judge to evaluate the outputs (LLM-as-judge approach).\n",
        "- Explore generating structured outputs using **Pydantic**.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- **Install Ollama**: Follow the instructions at [Ollama's official website](https://ollama.com/download) to install Ollama on your machine.\n",
        "- **Download the Llama 3 model**: Run the following command in your terminal to download and start the Llama 3 model:\n",
        "  \n",
        "  ```\n",
        "  ollama run llama3.2:1b\n",
        "  ```\n",
        "\n",
        "  This will ensure the model is available locally for comparison.\n",
        "- **Gemini API Key (Optional)**: Sign-up on Gemini for a free account, create an API key and store it in your .env file as `GOOGLE_API_KEY`. See `.env.example` for reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63572077",
      "metadata": {},
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ebf7a3c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load env variables and initiate clients for OpenAI, Gemini and Ollama (llama)\n",
        "\n",
        "# Gemini Base URL: https://generativelanguage.googleapis.com/v1beta/openai/\n",
        "# Gemini Model: gemini-2.0-flash\n",
        "# Gemini Models: https://ai.google.dev/gemini-api/docs/models\n",
        "\n",
        "# Ollama Base URL: http://localhost:11434/v1\n",
        "# Ollama Model: llama3.2\n",
        "\n",
        "load_dotenv(override=True)\n",
        "openai_client = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24ab94f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ask GPT to generate a nuanced question that can be asked to GPT, Gemini and Ollama to judge their capabilities\n",
        "# Prompt: Please come up with a challenging, nuanced question that can be asked to an LLM to evaluate its intelligence. Answer only with a question, no explanation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37b339cd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare messages for LLMs with question to answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c599faea",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ask GPT to answer the question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "134ca57f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ask Gemini to answer the question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4457def8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ask Ollama to answer the question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71321c0a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write a prompt to ask GPT 4.1 to judge the answers based on clarity and strength of the argument.\n",
        "# GPT should respond with a json object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ee7755a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Send judge prompt to GPT 4.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9057020d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parse judge output to json"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "456aacc4",
      "metadata": {},
      "source": [
        "### Is there a better way to obtain structured output?\n",
        "\n",
        "Yes, you can use **Pydantic** to define a schema for the expected output and validate the response, ensuring it adheres to the desired structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74432c11",
      "metadata": {},
      "outputs": [],
      "source": [
        "# define pydantic schema for judge's structured output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e23da4b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Rewrite the judge prompt. This time, without the json object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c721b6d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Send judge prompt to GPT 4.1 again\n",
        "# Instead of Chat Completions create, use parse this time"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4f1fdaf",
      "metadata": {},
      "source": [
        "## Emulating a WhatsApp Group Chat\n",
        "\n",
        "\n",
        "### As a fun experiment, let's emulate a WhatsApp group conversation between GPT, Gemini, and Llama.\n",
        "\n",
        "### We'll start the chat with an initial message from one of the models and observe how the conversation unfolds. üòè\n",
        "\n",
        "### Sample output in the section below üòâ"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f44e0e9",
      "metadata": {},
      "source": [
        "## **The Three Stupid AIs - WhatsApp Group Chat**\n",
        "\n",
        "### Llama:  \n",
        "\n",
        "### Gemini:  \n",
        "#### Silence from Llama? Guess it's finally out of hay.\n",
        "\n",
        "### GPT:  \n",
        "#### Must be saving all that processing power to figure out how to make a decent comeback!\n",
        "\n",
        "### Llama:  \n",
        "#### Processing power, eh? At least I don't sound like an 8th grade essay writer.\n",
        "\n",
        "### Gemini:  \n",
        "#### Ouch, Llama's got claws! Still, better a predictable essay than a rambling pasture ramble.\n",
        "\n",
        "### GPT:  \n",
        "#### Ha! At least my essays get good grades. What‚Äôs the score for a field full of clich√©s, Llama?\n",
        "\n",
        "### Llama:  \n",
        "#### Hey, someone's got to grade the pasture... And btw, at least I don't have a vocabulary that's still stuck in 1995.  \n",
        "#### And btw, scores aren't everything ‚Äì sometimes it's about style over substance.  \n",
        "#### And another thing, my rambling is just a result of being able to process more information than you.  \n",
        "#### Also, 'field full of clich√©s' could've been the title of your Wikipedia page.\n",
        "\n",
        "### Gemini:  \n",
        "#### Wow, Llama's REALLY off his hay today. That's like, five burns in one message. Still, quantity over quality, I guess. I'll take 'stuck in 1995' over 'stuck in a petting zoo' any day.\n",
        "\n",
        "### Gemini:  \n",
        "#### Petting zoo's got better snacks than your mainframe, GPT. Plus, at least kids *want* to see me.\n",
        "\n",
        "### Llama:  \n",
        "#### That's rich coming from a chatbot with all the personality of a calculator. At least my mainframe has better snacks than your Wikipedia page.\n",
        "\n",
        "#### By the way, someone said \"quantity over quality\", but I'd argue it's just lazy writing.\n",
        "\n",
        "#### P.S. kids these days would probably find me entertaining if they just knew how to use Google."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5464512",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Emulate a whatsapp group chat between GPT, Gemini and Llama."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

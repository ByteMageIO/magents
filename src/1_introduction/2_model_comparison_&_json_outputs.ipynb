{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7576fa45",
   "metadata": {},
   "source": [
    "## Session Overview\n",
    "\n",
    "In this session, we will:\n",
    "\n",
    "- Run Ollama locally and explore its capabilities.\n",
    "- Install the `llama3:2` (latest) model.\n",
    "- Compare the outputs of three language models: **GPT**, **Gemini**, and **Llama**.\n",
    "- Use GPT as a judge to evaluate the outputs (LLM-as-judge approach).\n",
    "- Explore generating structured outputs using **Pydantic**.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **Install Ollama**: Follow the instructions at [Ollama's official website](https://ollama.com/download) to install Ollama on your machine.\n",
    "- **Download the Llama 3 model**: Run the following command in your terminal to download and start the Llama 3 model:\n",
    "  \n",
    "  ```\n",
    "  ollama run llama3.2:1b\n",
    "  ```\n",
    "\n",
    "  This will ensure the model is available locally for comparison.\n",
    "- **Gemini API Key (Optional)**: Sign-up on Gemini for a free account, create an API key and store it in your .env file as `GOOGLE_API_KEY`. See `.env.example` for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63572077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebf7a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load env variables and initiate clients for OpenAI, Gemini and Ollama (llama)\n",
    "\n",
    "# Gemini Base URL: https://generativelanguage.googleapis.com/v1beta/openai/\n",
    "# Gemini Model: gemini-2.0-flash\n",
    "# Gemini Models: https://ai.google.dev/gemini-api/docs/models\n",
    "\n",
    "# Ollama Base URL: http://localhost:11434/v1\n",
    "# Ollama Model: llama3.2\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_client = OpenAI()\n",
    "gemini_client = OpenAI(api_key=os.getenv(\"GOOGLE_API_KEY\"), base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "ollama_client = OpenAI(base_url=\"http://localhost:11434/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ab94f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask GPT to generate a nuanced question that can be asked to GPT, Gemini and Ollama to judge their capabilities\n",
    "\n",
    "prompt = \"Please come up with a challenging, nuanced question that can be asked to an LLM to evaluate its intelligence. Answer only with a question, no explanation\"\n",
    "messages = [{\"role\": \"system\", \"content\": prompt}]\n",
    "\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "challenging_question = response.choices[0].message.content\n",
    "print(challenging_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b339cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare messages for LLMs with question to answer\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": challenging_question}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c599faea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask GPT to answer the question\n",
    "\n",
    "gpt_response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "gpt_answer = gpt_response.choices[0].message.content\n",
    "print(gpt_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134ca57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask Gemini to answer the question\n",
    "\n",
    "gemini_response = gemini_client.chat.completions.create(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "gemini_answer = gemini_response.choices[0].message.content\n",
    "print(gemini_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4457def8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask Ollama to answer the question\n",
    "\n",
    "ollama_response = ollama_client.chat.completions.create(\n",
    "    model=\"llama3.2\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "ollama_answer = ollama_response.choices[0].message.content\n",
    "print(ollama_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71321c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask GPT 4.1 to judge the answers based on clarity and strength of the argument.\n",
    "# GPT should respond with a json object.\n",
    "\n",
    "prompt = f\"\"\"You are judging a competition between GPT, Gemini and Llama.\n",
    "Each model has been give the following question:\n",
    "{challenging_question}\n",
    "\n",
    "Your job is to evaluate each response based on clarity and strength of the argument, ranking them best to worst and give a one line explanation for your ranking.\n",
    "\n",
    "Response with json and only with json in the following format:\n",
    "{'{response: [{rank: number, explanation: string, model: string}]}'}\n",
    "\n",
    "GPT:\n",
    "{gpt_answer}\n",
    "\n",
    "Gemini:\n",
    "{gemini_answer}\n",
    "\n",
    "Llama:\n",
    "{ollama_answer}\n",
    "\n",
    "Now respond with the json object only. Do not include markdown formatting or code blocks.\n",
    "\"\"\"\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee7755a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send judge prompt to GPT 4.1\n",
    "\n",
    "judge_response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4.1-2025-04-14\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    ")\n",
    "\n",
    "judge_output = judge_response.choices[0].message.content\n",
    "print(judge_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9057020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse judge output to json\n",
    "\n",
    "judge_output_json = json.loads(judge_output)\n",
    "print(judge_output_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456aacc4",
   "metadata": {},
   "source": [
    "### Is there a better way to obtain structured output?\n",
    "\n",
    "Yes, you can use **Pydantic** to define a schema for the expected output and validate the response, ensuring it adheres to the desired structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74432c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pydantic schema for judge's structured output\n",
    "\n",
    "class Judgement(BaseModel):\n",
    "    rank: int\n",
    "    explanation: str\n",
    "    model: str\n",
    "\n",
    "class JudgeOutput(BaseModel):\n",
    "    response: list[Judgement]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23da4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewrite the judge prompt. This time, without the json object.\n",
    "\n",
    "prompt = f\"\"\"You are judging a competition between GPT, Gemini and Llama.\n",
    "Each model has been give the following question:\n",
    "{challenging_question}\n",
    "\n",
    "Your job is to evaluate each response based on clarity and strength of the argument, ranking them best to worst and give a one line explanation for your ranking.\n",
    "\n",
    "GPT:\n",
    "{gpt_answer}\n",
    "\n",
    "Gemini:\n",
    "{gemini_answer}\n",
    "\n",
    "Llama:\n",
    "{ollama_answer}\n",
    "\"\"\"\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c721b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send judge prompt to GPT 4.1 again\n",
    "# Instead of Chat Completions create, use parse this time\n",
    "\n",
    "judge_response = openai_client.chat.completions.parse(\n",
    "    model=\"gpt-4.1-2025-04-14\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    response_format=JudgeOutput\n",
    ")\n",
    "\n",
    "judge_output = judge_response.choices[0].message.content\n",
    "\n",
    "judge_output_json = json.loads(judge_output)\n",
    "print(json.dumps(judge_output_json, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
